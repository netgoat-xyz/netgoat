import os
import json
import numpy as np
from flask import Flask, request, jsonify
from tensorflow.keras.models import load_model
import tensorflow as tf
from werkzeug.exceptions import BadRequest
from flask_cors import CORS

# --- Configuration ---
# NOTE: This model file must be generated by running network_traffic_model.py first!
MODEL_PATH = 'network_traffic_model.keras'
SCALER_PARAMS_PATH = 'scaler_params.json'
N_MONTE_CARLO_PREDICTIONS = 10
CLASS_LABELS = {
    0: 'Normal Traffic',
    1: 'Heavy Traffic (High Load)',
    2: 'Denial-of-Service (DoS) Attack',
    3: 'Distributed Denial-of-Service (DDoS) Attack'
}

# Global variables for the loaded model and scaler parameters
traffic_model = None
scaler_params = None
min_vals = None
max_vals = None

app = Flask(__name__)
CORS(app)

# --- Model Loading and Initialization ---

def load_model_and_scaler():
    """
    Loads the Keras model and the MinMax scaler parameters once on startup.
    """
    global traffic_model, scaler_params, min_vals, max_vals

    try:
        # Load the Keras H5 model
        print(f"Loading Keras model from {MODEL_PATH}...")
        traffic_model = load_model(MODEL_PATH)
        print("Model loaded successfully.")

        # Load scaler parameters
        print(f"Loading scaler parameters from {SCALER_PARAMS_PATH}...")
        with open(SCALER_PARAMS_PATH, 'r') as f:
            scaler_params = json.load(f)
            min_vals = np.array(scaler_params['min'])
            max_vals = np.array(scaler_params['max'])
        print("Scaler parameters loaded successfully.")

        # Ensure the model is ready by running a dummy prediction
        # Also ensures that TensorFlow is initialized correctly
        dummy_input = np.zeros((1, 5), dtype=np.float32)
        traffic_model.predict(dummy_input)
        
    except Exception as e:
        print(f"Error during initialization: {e}")
        raise e

# --- Core Logic: Scaling and Prediction ---

def scale_features_deploy(features_raw):
    """
    Applies the MinMax scaling transformation using the loaded min/max parameters.
    Formula: (X - min) / (max - min)
    """
    scaled = []
    
    if len(features_raw) != len(min_vals):
        raise ValueError(f"Expected {len(min_vals)} features but received {len(features_raw)}.")
    
    for i, value in enumerate(features_raw):
        min_val = min_vals[i]
        max_val = max_vals[i]
        
        # Guard against zero range (max == min)
        if max_val - min_val == 0:
            scaled.append(0.0)
        else:
            scaled.append((value - min_val) / (max_val - min_val))
            
    # Return as a NumPy array for the model
    return np.array([scaled])

def run_mc_dropout_prediction(scaled_features):
    """
    Performs Monte Carlo Dropout inference by running the model multiple times 
    with dropout layers active (training=True) and averaging the results.
    """
    mc_predictions = []
    
    # We use tf.function wrapping for potentially better performance, though Keras 
    # model.__call__ handles this efficiently too.
    @tf.function
    def predict_with_dropout(input_tensor):
        # The key for MC Dropout is setting training=True during inference
        return traffic_model(input_tensor, training=True)

    input_tensor = tf.convert_to_tensor(scaled_features, dtype=tf.float32)
    
    for _ in range(N_MONTE_CARLO_PREDICTIONS):
        # We collect the raw tensor output
        mc_predictions.append(predict_with_dropout(input_tensor).numpy())

    # Average the 10 predictions to get the final, stable probability distribution
    predictions_avg = np.mean(np.stack(mc_predictions), axis=0)[0]
    
    # Extract results
    predicted_class_index = int(np.argmax(predictions_avg))
    confidence = float(predictions_avg[predicted_class_index])
    predicted_label = CLASS_LABELS.get(predicted_class_index, 'Unknown')
    
    return predicted_label, confidence

# --- Flask API Route ---

@app.route('/predict', methods=['POST'])
def predict():
    """
    API endpoint to receive network traffic features and return a prediction.
    Expected JSON payload: {"features": [f1, f2, f3, f4, f5]}
    """
    if traffic_model is None or scaler_params is None:
        return jsonify({"error": "Model not loaded. Server is still initializing or failed to load assets."}), 503

    try:
        # 1. Get data from POST request
        data = request.get_json(force=True)
        if 'features' not in data or not isinstance(data['features'], list) or len(data['features']) != 5:
             raise BadRequest("Invalid input format. Expected JSON: {\"features\": [f1, f2, f3, f4, f5]} with 5 numerical values.")
        
        raw_features = [float(x) for x in data['features']]

        # 2. Scale the input
        scaled_features = scale_features_deploy(raw_features)

        # 3. Perform MC Dropout prediction
        predicted_label, confidence = run_mc_dropout_prediction(scaled_features)
        
        # 4. Generate reasoning (based on Python script logic)
        reasoning = ""
        predicted_class_index = [k for k, v in CLASS_LABELS.items() if v == predicted_label][0]

        if predicted_class_index == 3:
            reasoning = "The combination of extreme packet count, low duration, high IP entropy, and high fake IP ratio strongly indicates a coordinated, distributed attack (DDoS)."
        elif predicted_class_index == 2:
            reasoning = "Extreme packet count, low duration, and very low IP entropy (suggesting a single source or small number of sources) points to a standard denial-of-service (DoS) attack."
        elif predicted_class_index == 1:
            reasoning = "High flow rate over a longer duration with low fake IP ratio is typical of legitimate heavy traffic (e.g., large file transfer, video streaming)."
        elif predicted_class_index == 0:
            reasoning = "Low packet count, long duration, and low flow rate are hallmarks of routine, normal network activity."

        # 5. Return JSON response
        response = {
            "status": "success",
            "prediction": predicted_label,
            "confidence": f"{confidence * 100:.2f}%",
            "raw_features": raw_features,
            "reasoning": reasoning
        }
        return jsonify(response)

    except BadRequest as e:
        return jsonify({"status": "error", "message": str(e)}), 400
    except Exception as e:
        # Catch unexpected errors during processing
        print(f"Internal processing error: {e}")
        return jsonify({"status": "error", "message": "Internal server error during prediction."}), 500

# --- Server Startup ---

if __name__ == '__main__':
    # Load model and scaler before starting the Flask server
    load_model_and_scaler()
    print("\n--- Starting Flask Server ---")
    # Run server on localhost:5000 (default Flask port)
    app.run(debug=True, host='0.0.0.0', port=5000)